syntax = "proto3";

package orama_ai_service;

service LLMService {
    rpc CheckHealth (HealthCheckRequest) returns (HealthCheckResponse);
    rpc GetEmbedding (EmbeddingRequest) returns (EmbeddingResponse);
    rpc Chat (ChatRequest) returns (ChatResponse);
    rpc ChatStream (ChatRequest) returns (stream ChatStreamResponse);
    rpc PlannedAnswer (PlannedAnswerRequest) returns (stream PlannedAnswerResponse);
}

enum OramaModel {
    BGESmall = 0;
    BGEBase = 1;
    BGELarge = 2;
    MultilingualE5Small = 3;
    MultilingualE5Base = 4;
    MultilingualE5Large = 5;
}

enum OramaIntent {
    query = 0;
    passage = 1;
}

enum LLMType {
    content_expansion = 0;
    google_query_translator = 1;
    vision = 2;
    answer = 3;
}

enum Role {
  USER = 0;
  ASSISTANT = 1;
  SYSTEM = 2;
}

message ConversationMessage {
  Role role = 1;
  string content = 2;
}

message Conversation {
  repeated ConversationMessage messages = 1;
}

// Request message for embedding generation
message EmbeddingRequest {
    OramaModel model = 1;         // The model to use for embedding
    repeated string input = 2;    // Array of input strings
    OramaIntent intent = 3;       // Optional intent description
}

// Response message for embedding generation
message EmbeddingResponse {
    repeated Embedding embeddings_result = 1; // Array of embeddings
    int32 dimensions = 2;                     // Dimensions of each embedding vector
}

// A single embedding representation
message Embedding {
    repeated float embeddings = 1; // Array of float values
}

// Request message for a planned answer
message PlannedAnswerRequest {
    string input = 1; // The user input
    string collection_id = 2; // ID of the collection
    Conversation conversation = 3; // All past messages
}

message PlannedAnswerResponse {
    string data = 1;
}

// Request message for LLM calls
message ChatRequest {
    LLMType model = 1;             // Which LLM to use
    string prompt = 2;             // Input prompt
    Conversation conversation = 3; // All past messages
    optional string context = 4;
}

// Response message for LLM calls
message ChatResponse {
    string text = 1;             // Generated text response
}

// Response message for streaming LLM calls
message ChatStreamResponse {
    string text_chunk = 1;       // Chunk of generated text
    bool is_final = 2;           // Indicates if this is the final chunk
}

// Request message for health check
message HealthCheckRequest {
    string service = 1;          // Service name
}

// Response message for health check
message HealthCheckResponse {
    string status = 1;           // Service status
}