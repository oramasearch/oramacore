# Not using networks as they seems to break on WSLv2

services:
  oramacore:
    build:
      context: .
    environment:
      - RUST_LOG=oramacore=trace,warn
    volumes:
      - ./config-docker.yaml:/app/config.yaml
    ports:
      - "8080:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    depends_on:
      - python-ai-server
      - vllm
    restart: unless-stopped

  vllm:
    image: vllm/vllm-openai:v0.8.3
    command: --model Qwen/Qwen2.5-3B-Instruct --host 0.0.0.0 --port 8000 --enable-auto-tool-choice --tool-call-parser hermes
    ports:
      - "8000:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: unless-stopped

  envoy:
    image: envoyproxy/envoy:v1.26-latest
    ports:
      - "80:80"
      - "9901:9901"
    volumes:
      - ./envoy/envoy.yaml:/etc/envoy/envoy.yaml
    depends_on:
      - oramacore
      - python-ai-server
      - vllm
    restart: unless-stopped
