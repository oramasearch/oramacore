http:
    host: 0.0.0.0
    port: 8080
    allow_cors: true
    with_prometheus: true

log:
    # Comment the following line to disable the logging
    file_path: "./log.log"
    levels:
        oramacore: trace

writer_side:
    output:
        type: in-memory
        # type: rabbitmq
        # host: localhost
        # port: 5552
        # user: guest
        # password: guest
        # v_host: /
        # stream_name: oramacore-operations
        # client_provided_name: oramacore-producer
        # producer_name: write
        # initial_offset: 0  # Default offset value for the first time
        # type: kafka
        # brokers: localhost:9092
        # topic: oramacore-operations
        # client_id: oramacore-producer
        # initial_offset: 0  # Default offset value for the first time

    hooks:
        select_embeddings_properties:
            check_interval: 60s
            max_idle_time: 20s
            instances_count_per_code: 3
            queue_capacity: 100
            max_execution_time: 1s
            max_startup_time: 100ms

    # Replace the following value with your own API key
    master_api_key: my-master-api-key
    config:
        data_dir: ./.data/writer
        # The maximum number of embeddings that can be stored in the queue
        # before the writer starts to be blocked
        # NB: the elements are in memory, so be careful with this value
        embedding_queue_limit: 50000
        # The number of the document insertions after the write side will commit the changes
        insert_batch_commit_size: 50000000
        # The default embedding model used to calculate the embeddings
        # if not specified in the collection creation
        default_embedding_model: BGESmall
        # The maximum number of request to javascript runtime that can be stored in the queue
        # NB: the elements are in memory, so be careful with this value
        javascript_queue_limit: 500000
        # Set interval for commiting the changes to the disk
        commit_interval: 1m
        # Temporary index cleanup configuration
        temp_index_cleanup:
            # How often to run cleanup (default: 1 hour)
            cleanup_interval: 1h
            # Maximum age before cleanup (default: 12 hours)
            max_age: 12h
            # Enable/disable cleanup (default: true)
            enabled: true
    # Uncomment to allow JWT validation
    # jwt:
    #     jwks_url: http://localhost:3000/api/.well-known/jwks.json
    #     issuers:
    #         - http://localhost:3000
    #     audiences:
    #         - http://localhost:8080


reader_side:
    # Optional on the reader side
    master_api_key: my-master-api-key

    input:
        type: in-memory
        # type: rabbitmq
        # host: localhost
        # port: 5552
        # user: guest
        # password: guest
        # v_host: /
        # stream_name: oramacore-operations
        # client_provided_name: oramacore-producer
        # consumer_name: reader
        # initial_offset: 0  # Default offset value for the first time
        # type: kafka
        # brokers: localhost:9092
        # topic: oramacore-operations
        # group_id: oramacore-consumer-group
        # client_id: oramacore-consumer
        # initial_offset: 0  # Default offset value for the first time

    config:
        data_dir: ./.data/reader
        # The number of the write operation after the read side will commit the changes
        insert_batch_commit_size: 50000000
        # Set interval for commiting the changes to the disk
        commit_interval: 1m

        # offload fields if there's no search on that field in the last 30minutes
        # keeping a window of 2 ^ 4 * 2 ^ 8 seconds = 4096 seconds = ~1 hour
        offload_field:
            unload_window: 30m
            # 2 ^ 8 groups
            slot_count_exp: 8
            # group by 2 ^ 4 seconds
            slot_size_exp: 4

    # Uncomment to allow analytics
    analytics:
        api_key: my-analytics-api-key
        metadata_from_headers:
            -   header: user-agent
                metadata_key: ua
            -   header: CF-Connecting-IP
                metadata_key: ip
            -   header: CF-IPCountry
                metadata_key: country
            -   header: continent
                metadata_key: continent
            -   header: colo
                metadata_key: colo
            -   header: latitude
                metadata_key: latitude
            -   header: longitude
                metadata_key: longitude

ai_server:
    scheme: http
    host: 0.0.0.0
    port: 50051
    api_key: ""
    max_connections: 15
    total_threads: 12

    embeddings:
        default_model_group: small
        dynamically_load_models: false
        execution_providers:
            #- CUDAExecutionProvider
            - CPUExecutionProvider
        total_threads: 8
        # automatic_embeddings_selector:
        #     model: "gpt-4.1"
        #     provider: openai

    llm:
        local: true
        port: 8000
        host: 0.0.0.0
        model: "Qwen/Qwen2.5-3B-Instruct"

      # Remote LLM configuration
      # Uncomment the lines below and comment the above to use remote LLM
      # local: false
      # host: "https://api.groq.com/openai/v1"
      # model: "openai/gpt-oss-120b"
      # api_key: "your-api-key-here"


    remote_llms:
    # - provider: openai
    #   api_key: sk-
    #   default_model: "gpt-4.1"
    # - provider: together
    #   api_key: "sk-"
    #   default_model: "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF"
    # - provider: fireworks
    #   api_key: "sk-"
    #   default_model: "accounts/fireworks/models/llama-v3p1-8b-instruct"
