http:
    host: 0.0.0.0
    port: 8080
    allow_cors: true
    with_prometheus: true

writer_side:
    output: in-memory
    config:
        data_dir: ./.data/writer
        # The maximum number of embeddings that can be stored in the queue
        # before the writer starts to be blocked
        # NB: the elements are in memory, so be careful with this value
        embedding_queue_limit: 50
        # The number of the document insertions after the write side will commit the changes
        insert_batch_commit_size: 1000
        # The default embedding model used to calculate the embeddings
        # if not specified in the collection creation
        default_embedding_model: MultilingualE5Small

reader_side:
    input: in-memory
    config:
        data_dir: ./.data/reader
        # The number of the write operation after the read side will commit the changes
        insert_batch_commit_size: 300

ai_server:
    scheme: http
    host: 0.0.0.0
    port: 50051
    api_key: ""
    max_connections: 15
    total_threads: 12

    embeddings:
        default_model_group: small
        dynamically_load_models: false
        execution_providers:
            - CUDAExecutionProvider
            - CPUExecutionProvider
        total_threads: 8
    LLMs:
        content_expansion:
            id: "Qwen/Qwen2.5-3B-Instruct"
            tensor_parallel_size: 1
            use_cpu: false
            sampling_params:
                temperature: 0.2
                top_p: 0.95
                max_tokens: 256
        google_query_translator:
            id: "Qwen/Qwen2.5-3B-Instruct"
            tensor_parallel_size: 1
            use_cpu: false
            sampling_params:
                temperature: 0.2
                top_p: 0.95
                max_tokens: 20
        answer:
            id: "Qwen/Qwen2.5-3B-Instruct"
            tensor_parallel_size: 1
            use_cpu: false
            sampling_params:
                temperature: 0
                top_p: 0.95
                max_tokens: 2048
